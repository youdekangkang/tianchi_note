此笔记为阿里云天池机器学习训练营笔记，学习地址：https://tianchi.aliyun.com/s/1fc36a7e103eb9948c974f638e83a83b
感谢教程及阿里云提供平台

### 1.学习知识点概要

#### XGBoost是什么

XGBoost是2016年由华盛顿大学陈天奇老师带领开发的一个可扩展机器学习系统。严格意义上讲XGBoost并不是一种模型，而是一个可供用户轻松解决分类、回归或排序问题的软件包。它内部实现了梯度提升树(GBDT)模型，并对模型中的算法进行了诸多优化，在取得高精度的同时又保持了极快的速度，在一段时间内成为了国内外数据挖掘、机器学习领域中的大规模杀伤性武器。

**优点：**

1. 简单易用。相对其他机器学习库，用户可以轻松使用XGBoost并获得相当不错的效果。
2. 高效可扩展。在处理大规模数据集时速度快效果好，对内存等硬件资源要求不高。
3. 鲁棒性强。相对于深度学习模型不需要精细调参便能取得接近的效果。
4. XGBoost内部实现提升树模型，可以自动处理缺失值。

**缺点：**

1. 相对于深度学习模型无法对时空位置建模，不能很好地捕获图像、语音、文本等高维数据。
2. 在拥有海量训练数据，并能找到合适的深度学习模型时，深度学习的精度可以遥遥领先XGBoost。

### 2.学习内容

#### 学习目标：

- 了解 XGBoost 的参数与相关知识
- 掌握 XGBoost 的Python调用并将其运用到天气数据集预测

#### 代码流程：

- Step1: 库函数导入
- Step2: 数据读取/载入
- Step3: 数据信息简单查看
- Step4: 可视化描述
- Step5: 对离散变量进行编码
- Step6: 利用 XGBoost 进行训练与预测
- Step7: 利用 XGBoost 进行特征选择
- Step8: 通过调整参数获得更好的效果

#### 独热编码

由于XGBoost不能处理字符串类型的数据,所以需要首先将这种类型的数据进行转换

独热编码 是利用0和1表示一些参数，例如女=0，男=1，狗狗=2，所以最后编码的特征值是在\[0 , 特征数量-1\]之间的整数。除此之外，还有独热编码、求和编码、留一法编码等等方法可以获得更好的效果。

参考数字手写体识别中：

如数字字体识别0~9中，6的独热编码为

0000001000

实现：

```python
# 导入依赖
from sklearn import preprocessing

encoded_vector = encoder.transform([[2, 3, 5, 3]]).toarray()
```



#### XGBoost基本参数

所有资料可以查阅xgboost官方文档：

[Python API Reference — xgboost 1.5.1 documentation](https://xgboost.readthedocs.io/en/stable/python/python_api.html)

```python
class xgboost.XGBClassifier(*, objective='binary:logistic', use_label_encoder=True, **kwargs)
```

**- 模型参数**

- n_estimatores
  - 含义：总共迭代的次数，即决策树的个数
- max_depth
  - 含义：树的深度，默认值为6，典型值3-10。和GBM中的参数相同，这个值为树的最大深度。 这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。 需要使用CV函数来进行调优。
  - 调参：值越大，越容易过拟合；值越小，越容易欠拟合。
- 

**- 学习任务参数**

- learning_rate

  - 含义：学习率，控制每次迭代更新权重时的步长，默认0.3。
  - 调参：值越小，训练越慢。
  - 典型值为0.01-0.2。

- objective 目标函数

  - **回归任务**

    - reg:linear (默认)
    - reg:logistic

  - 二分类

    - binary:logistic   概率 
    - binary：logitraw  类别

  - 多分类

    - multi：softmax num_class=n  返回类别
    - multi：softprob  num_class=n 返回概率

  - rank:pairwise

  - eval_metric

    - 校验数据所需要的评价指标，不同的目标函数将会有缺省的评价指标（rmse for regression, and error for classification, mean average precision for ranking）-

    - 用户可以添加多种评价指标，对于[Python](https://so.csdn.net/so/search?from=pc_blog_highlight&q=Python)用户要以list传递参数对给程序，而不是map参数list参数不会覆盖’eval_metric’

    - 可供的选择如下:

    - 回归任务(默认rmse)

    - rmse--均方根误差

    - mae--平均绝对误差

      - 分类任务(默认error)

        - auc--roc曲线下面积

          - error--错误率（二分类）
          - merror--错误率（多分类）
          - logloss--负对数似然函数（二分类）
          - mlogloss--负对数似然函数（多分类）

          

  - gamma

    - 惩罚项系数，指定节点分裂所需的最小损失函数下降值。在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。
    - 调参：

  - alpha

    - L1正则化系数，默认为1

  - lambda

    - L2正则化系数，默认为1 
    - 用于处理XGBoost的正则化部分。通常不使用，但可以用来降低过拟合

  - lambda_bias

    - 在偏置上的L2正则。缺省值为0（在L1上没有偏置项的正则，因为L1偏置时不需要）

  - subsample

    - 系统默认为1。这个参数控制对于每棵树，随机采样的比例。减小这个参数的值，算法会更加保守，避免过拟合, 取值范围零到一。

  - colsample_bytree

    - 系统默认值为1。我们一般设置成0.8左右。用来控制每棵随机采样的列数的占比(每一列是一个特征)。

**# 代码主要函数：**

- 载入数据：load_digits()
- 数据拆分：train_test_split()
- 建立模型：XGBClassifier()
- 模型训练：fit()
- 模型预测：predict()
- 性能度量：accuracy_score()
- 特征重要性：plot_importance()

### 3.问题与解决

**1.数据不平衡是什么？**

回答：又称样本比例失衡，比如二分类问题，如果标签为1的样本占总数的99%，标签为0的样本占比1%则会导致判断**「失误严重」**，准确率虚高。

**2.数据预处理的基本步骤是什么**

回答:**数据清理–>数据集成 —>数据归约–>数据变换**

（1）数据清洗 —— 去噪声和无关数据
（2）数据集成 —— 将多个数据源中的数据结合起来存放在一个一致的数据存储中
（3）数据规约 —— 主要方法包括：数据立方体聚集，维度归约，数据压缩，数值归约，离散化和概念分层等。
（4）数据变换 —— 把原始数据转换成为适合数据挖掘的形式

**数据清洗的路子：刚拿到的数据 —-> 和数据提供者讨论咨询 —–> 数据分析（借助可视化工具）发现脏数据 —->清洗脏数据（借助MATLAB或者Java/C++语言） —–>再次统计分析（Excel的data analysis不错的，最大小值，中位数，众数，平均值，方差等等，以及散点图） —–> 再次发现脏数据或者与实验无关的数据（去除） —–>最后实验分析 —-> 社会实例验证 —->结束。**

### 4.归纳与总结

XGBoost讲起来还有点复杂，刚开始看算法的时候也是一愣一愣的。
实际上它就是一堆二叉树，准确来讲是CART树，和GBDT一样，在GBDT中，无论是分类还是回归，也都是一堆CART树。当然xgboost还支持其它的基分类器。

