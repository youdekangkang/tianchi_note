此笔记为阿里云天池机器学习训练营笔记，学习地址：https://tianchi.aliyun.com/s/1fc36a7e103eb9948c974f638e83a83b
感谢教程及阿里云提供平台

### 一、赛题说明

赛题使用公开数据的问卷调查结果，选取其中多组变量，包括个体变量（性别、年龄、地域、职业、健康、婚姻与政治面貌等等）、家庭变量（父母、配偶、子女、家庭资本等等）、社会态度（公平、信用、公共服务等等），来预测其对幸福感的评价。

### 二、数据说明
考虑到变量个数较多，部分变量间关系复杂，数据分为完整版和精简版两类。先从精简版入手熟悉赛题后，使用完整版挖掘更多信息。complete文件为变量完整版数据，abbr文件为变量精简版数据。index文件中包含每个变量对应的问卷题目，以及变量取值的含义。
survey文件是数据源的原版问卷，作为补充以方便理解问题背景。

总计共有6个文件，[点击查看]([Happinese_survey | Kaggle](https://www.kaggle.com/zhuhaibin/happinese-survey))

这里我们就直接上手完整版数据集了

### 三、 数据预处理

将测试集连接在一起

```python
# 将train和test连在一起
data = pd.concat([train,test],axis=0,ignore_index=True)
```

#### 缺失值的处理

对于缺失值，主要的处理方法即为删除与填补

首先需要使用head查看数据大致长相

```python
train.head()
```

然后使用info或者describe函数查看数据分布

```python
#verbose 表示是否打印完整信息
#show_counts表示数据个数 nan不计入
train.info(verbose=True,show_counts=True)

train.describe()
```

一般会发现以下两种情况 

- 数据个数不同（缺少数据）

- 数据类型不对（无法识别字符串类型）
- 数据错误（比如本来可能应该是正的，但是数据集里有某几个负值）

如果某个字段对于目标字段的相关性不大，那么就可以考虑把这个字段丢弃了

丢弃字段：

```python
# 去掉"happiness"、"survey_time"、"id"
data=data.drop(["happiness"], axis=1)
data=data.drop(["survey_time"], axis=1)
data=data.drop(["id"], axis=1)
```

处理异常值：

```python
# 将异常值替换为NaN
data.replace([-1, -2, -3, -8], np.nan, inplace=True)

# 填充数据——缺失值
# 通过观察特征可以发现大多数类别特征的类别数在5个以内，以5为阈值初步区分数字特征与类别特征

data_cat = data.loc[:, list(data.max()<=5)]
data_num = data.loc[:, list(data.max()>5)]

imputer_cat = SimpleImputer(strategy="most_frequent")
imputer_num = SimpleImputer(strategy="median")
imputer_cat.fit(data_cat)
imputer_cat_trans = imputer_cat.transform(data_cat)
imputer_num.fit(data_num)
imputer_num_trans = imputer_num.transform(data_num)
data[data_cat.columns] = imputer_cat_trans
data[data_num.columns] = imputer_num_trans
```

**SimpleImputer**

这个类用来填充数据中的缺失值

```python
from sklearn.Imputer import SimpleImputer
```

- **missing_values**,也就是缺失值是什么，一般情况下缺失值当然就是[空值](https://www.zhihu.com/search?q=空值&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A83173703})啦，也就是np.nan

- **strategy**:也就是你采取什么样的策略去填充空值，总共有4种选择。分别是mean,median, most_frequent,以及constant，这是对于每一列来说的，如果是mean，则该列则由该列的均值填充。而median,则是中位数，most_frequent则是[众数](https://www.zhihu.com/search?q=众数&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A83173703})。需要注意的是，如果是constant,则可以将空值填充为自定义的值，这就要涉及到后面一个参数了，也就是[fill_value](https://www.zhihu.com/search?q=fill_value&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A83173703})。如果strategy='constant',则填充fill_value的值。

- **copy**:则表示对原来没有填充的数据的拷贝。

- **add_indicator**:如果该参数为True，则会在数据后面加入n列由0和1构成的同样大小的数据，0表示所在位置非空，1表示所在位置为空。相当于一种判断是否为空的索引。

#### 字段处理

处理时间特征

```python
# 处理时间特征
data['survey_time'] = pd.to_datetime(data['survey_time'],format='%Y/%m/%d %H:%M:%S')
data["weekday"]=data["survey_time"].dt.weekday
data["year"]=data["survey_time"].dt.year
data["quarter"]=data["survey_time"].dt.quarter
data["hour"]=data["survey_time"].dt.hour
data["month"]=data["survey_time"].dt.month
```

处理字符串特征

```python
# 是否入党
data["join_party"]=data["join_party"].map(lambda x:0 if pd.isnull(x)  else 1)

# 填充数据——非数据值
data["edu_other"]=data["edu_other"].map(lambda x:0 if pd.isnull(x) else 1)
data["property_other"]=data["property_other"].map(lambda x:0 if pd.isnull(x) else 1)
data["invest_other"]=data["invest_other"].map(lambda x:0 if pd.isnull(x) else 1)
```

得到所有结果后再次使用info，一定要保证每一个字段计数的数量相同

**独热编码转换**

```python
# 将类别特征中特征值与特征含义没有明显线性关系的特征取出并进行one-hot编码处理

cat_features = data.loc[:, ['survey_type', 'province', 'city', 'county', 'gender', 'nationality', 'edu_status', 'political', 'hukou', 'hukou_loc', 'work_exper', 'work_status', 'work_type', 'work_manage', 'insur_1', 'insur_2', 'insur_3', 'insur_4', 'car', 'marital', 's_hukou', 's_work_status', 's_work_type', 'f_political', 'f_work_14', 'm_political', 'm_work_14']]
```

```python
#独热编码转换
encoder = OneHotEncoder()
cat_features_1hot = encoder.fit_transform(cat_features.values)

cat_features_1hot = cat_features_1hot.toarray()
```

### 四、模型建立

这里我是用XGBoost进行处理

```python
model_2 = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.1,
       colsample_bytree=0.971, gamma=0.11, learning_rate=0.069, max_delta_step=0,
       max_depth=3, min_child_weight=1, n_estimators=499,
       n_jobs=-1, nthread=50, objective ='reg:squarederror', random_state=0,
       reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1, seed=None, subsample=1.0)
       
from sklearn.model_selection import cross_val_score

#K折痕交叉验证
scores = cross_val_score(model_2, X_train, y_train, 
                         scoring="neg_mean_squared_error", cv=10)
                         
model_2.fit(X_train, y_train)
X_predict = model_2.predict(X_test)

test_sub["happiness"] = X_predict
test_sub.to_csv("submit_lgbm.csv", index=False)
```

最终得分是0.46950 排名不是特别靠前 参数还能够进一步进行优化



### 五、 总结

这是第一次使用模型参加比赛，参考了别人的kernel发现高分模型往往不会手动调参，并且在得到结果之后还会继续优化，也就是需要结合多种模型。而且不止xgboost和lightboost，还有更多例如catboost等模型需要学习，任重而道远吧。

